{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in Index(['spotPrice'], dtype='object') : 6\n",
      "Missing values in Index(['ConsumptionGWh', 'ProductionGWh'], dtype='object') : 14\n",
      "Missing values in Index(['waterSE1(MWh)', 'windSE1(MWh)', 'trmSE1MWh'], dtype='object') : 46\n",
      "Missing values in Index(['waterSE2(MWh)', 'windSE2(MWh)', 'solSE2MWh', 'trmSE2MWh', 'OthSE2MWh'], dtype='object') : 144\n",
      "Missing values in Index(['waterSE3(MWh)', 'windSE3(MWh)', 'nucSE3(MWh)', 'solSE3MWh',\n",
      "       'trmSE3MWh', 'OthSE3MWh'],\n",
      "      dtype='object') : 139\n",
      "Missing values in Index(['waterSE4(MWh)', 'windSE4(MWh)', 'solSE4MWh', 'trmSE4MWh', 'OthSE4MWh'], dtype='object') : 155\n",
      "Missing values in Index(['TDK1SE3', 'TNO1SE3', 'TSE2SE3', 'TF1SE3', 'TSE4SE3'], dtype='object') : 0\n",
      "Missing values in Index(['TSE3DK1', 'TSE3NO1', 'TSE3SE2', 'TSE3F1', 'TSE3SE4'], dtype='object') : 0\n",
      "Missing values in Index(['F_LOWSE2SE3', 'F_LOWSE3FI', 'F_LOWSE3SE4'], dtype='object') : 6289\n",
      "Missing values in Index(['C_SE4SE3', 'C_DK1SE3', 'C_FISE3', 'C_NO1SE3', 'C_SE2SE3'], dtype='object') : 30\n",
      "Missing values in Index(['C_SE3DK1', 'C_SE3FI', 'C_SE3NO1', 'C_SE3SE2', 'C_SE3SE4'], dtype='object') : 30\n",
      "Missing values in Index(['ESE3_FI', 'ESE3_NO1', 'ESE3_DK1', 'ESE3_SE4', 'ESE3_SE2'], dtype='object') : 277\n",
      "Missing values in Index(['EFI_SE3', 'EDK1_SE3', 'ESE2_SE3', 'ESE4_SE3', 'ENO1_SE3'], dtype='object') : 277\n",
      "Missing values in Index(['TurnoverB', 'TurnoverS'], dtype='object') : 12\n",
      "Missing values in Index(['Temperature(day)', 'PrecipitationEnergySE(day)'], dtype='object') : 24\n",
      "Missing values in Index(['Wind(Pite)'], dtype='object') : 216\n",
      "Missing values in Index(['HydroRes(GWh_week)'], dtype='object') : 0\n"
     ]
    }
   ],
   "source": [
    "import datastore\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "spot = datastore.getSpotPrice()\n",
    "\n",
    "total_consumption_production = datastore.getTotalConsumptionProduction()\n",
    "\n",
    "production_se1_mwh = datastore.getAllSE1ProductionMWh() \n",
    "production_se2_mwh = datastore.getAllSE2ProductionMWh()\n",
    "production_se3_mwh = datastore.getAllSE3ProductionMWh()\n",
    "production_se4_mwh = datastore.getAllSE4ProductionMWh()\n",
    "\n",
    "transTo = datastore.getTransmissionCapTo()\n",
    "transFrom = datastore.getTransmissionCapFrom()\n",
    "\n",
    "# netFlow = datastore.getNetFlow() # Too many missing values\n",
    "flow = datastore.getFlow()\n",
    "\n",
    "capTo = datastore.getFlowCapacityTo()\n",
    "capFrom = datastore.getFlowCapacityFrom()\n",
    "\n",
    "exchangeFrom = datastore.getExchangeFrom()\n",
    "exchangeTo = datastore.getExchangeTo() \n",
    "\n",
    "turnOver = datastore.getTurnover()\n",
    "\n",
    "temp_pen = datastore.getDailyWeather() # DAILY celsius\n",
    "wind_velocities = datastore.getHourlyWindVelocity() # m/s\n",
    "hydroReservoirs = datastore.getWeeklyHydroReservs() # GWh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 54024 entries, 2018-01-01 00:00:00 to 2024-02-29 23:00:00\n",
      "Columns: 115 entries, spotPrice to group_ids\n",
      "dtypes: float64(61), int32(2), int64(1), object(1), uint8(50)\n",
      "memory usage: 31.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import calendar_features as cf\n",
    "\n",
    "dataset = pd.concat([spot, total_consumption_production,\n",
    "                     production_se1_mwh, production_se2_mwh, production_se3_mwh, production_se4_mwh, \n",
    "                     transTo, transFrom, \n",
    "                     flow, capTo, capFrom, exchangeFrom, exchangeTo,\n",
    "                     turnOver, wind_velocities, hydroReservoirs, temp_pen], axis=1)\n",
    "\n",
    "dataset.interpolate(method = 'linear', limit_direction = 'forward', inplace=True, axis=0)\n",
    "\n",
    "# daylight_features = cf.daylight_extractor(dataset)\n",
    "holiday_features = cf.get_holidays(dataset)\n",
    "dataset = cf.calendar_transformer(dataset)\n",
    "\n",
    "dataset = pd.concat([dataset, holiday_features], axis=1)\n",
    "dataset = pd.get_dummies(dataset, columns=[\"year\", \"month\", \"day_of_week\", \"hour\"])\n",
    "\n",
    "# add index \n",
    "dataset[\"time_idx\"] = range(len(dataset))\n",
    "dataset[\"time_idx\"] = dataset[\"time_idx\"].astype(int)\n",
    "dataset[\"group_ids\"] = \"SE3\" # add dummy variable for group_ids, only one called \"SE3\"\n",
    "\n",
    "\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "def create_sliding_window(data, sequence_length, stride=1):\n",
    "    X_list, y_list = [], []\n",
    "    for i in range(len(data)):\n",
    "      if (i + sequence_length) < len(data):\n",
    "        X_list.append(data.iloc[i:i+sequence_length:stride, :].values)\n",
    "        y_list.append(data.iloc[i+sequence_length, 0])\n",
    "    return np.array(X_list), np.array(y_list)\n",
    "  \n",
    "\n",
    "x = dataset.drop('spotPrice', axis=1)\n",
    "y = dataset['spotPrice']\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "train_split = '2021-12-31 23:00:00'\n",
    "val_split = '2022-12-31 23:00:00'\n",
    "test_split = '2024-02-29 23:00:00'\n",
    "\n",
    "train_split_index = dataset.index.get_loc(train_split)\n",
    "val_split_index = dataset.index.get_loc(val_split)\n",
    "test_split_index = dataset.index.get_loc(test_split)\n",
    "\n",
    "x_train = x[:train_split_index]\n",
    "y_train = y[:train_split_index]\n",
    "\n",
    "x_val = x[train_split_index:val_split_index]\n",
    "y_val = y[train_split_index:val_split_index]\n",
    "\n",
    "x_test = x[val_split_index:test_split_index]\n",
    "y_test = y[val_split_index:test_split_index]\n",
    "\n",
    "x_scaler.fit(x_train)\n",
    "y_scaler.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "x_train = x_scaler.transform(x_train)\n",
    "y_train = y_scaler.transform(y_train.values.reshape(-1, 1))\n",
    "\n",
    "x_val = x_scaler.transform(x_val)\n",
    "y_val = y_scaler.transform(y_val.values.reshape(-1, 1))\n",
    "\n",
    "x_test = x_scaler.transform(x_test)\n",
    "y_test = y_scaler.transform(y_test.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "# Combine x_train and y_train into a single pandas dataframe\n",
    "train_df = pd.concat([pd.DataFrame(x_train), pd.DataFrame(y_train, columns=[\"spotPrice\"])], axis=1)\n",
    "val_df = pd.concat([pd.DataFrame(x_val), pd.DataFrame(y_val, columns=[\"spotPrice\"])], axis=1)\n",
    "test_df = pd.concat([pd.DataFrame(x_test), pd.DataFrame(y_test, columns=[\"spotPrice\"])], axis=1)\n",
    "\n",
    "x_train, y_train = create_sliding_window(train_df, 240)\n",
    "x_val, y_val = create_sliding_window(val_df, 240)\n",
    "x_test, y_test = create_sliding_window(test_df, 240)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    " \n",
    "max_prediction_length = 24 * 5\n",
    "max_encoder_length = 24 * 5 * 2\n",
    "\n",
    "training_cutoff = dataset[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "target = \"spotPrice\"\n",
    "\n",
    "static_categoricals = [\"group_ids\"]\n",
    "\n",
    "time_varying_known_categoricals = dataset.iloc[:, 61:113].columns.to_list()\n",
    "dataset[time_varying_known_categoricals] = dataset[time_varying_known_categoricals].astype(str).astype(\"category\")\n",
    "\n",
    "\n",
    "\n",
    "time_varying_known_reals = [\"time_idx\", \"PrecipitationEnergySE(day)\", \"Temperature(day)\", \"HydroRes(GWh_week)\", \"Wind(Pite)\"]\n",
    "time_varying_unknown_reals = dataset.iloc[:, 0:57].columns.to_list()\n",
    "\n",
    "features = dataset.drop(columns=[target, \"time_idx\"]).columns.to_list()\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    dataset[lambda x: x.time_idx <= training_cutoff],\n",
    "    \n",
    "    time_idx=\"time_idx\",\n",
    "    \n",
    "    target=target,\n",
    "    \n",
    "    group_ids= static_categoricals,\n",
    "    \n",
    "    min_encoder_length=max_encoder_length,  \n",
    "    max_encoder_length=max_encoder_length,\n",
    "    \n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    \n",
    "    static_categoricals=static_categoricals,\n",
    "    static_reals=[],\n",
    "    \n",
    "    time_varying_known_categoricals= [],\n",
    "    time_varying_known_reals= [],\n",
    "    \n",
    "    time_varying_unknown_categoricals = [],\n",
    "    time_varying_unknown_reals= time_varying_unknown_reals,\n",
    "    \n",
    "    add_relative_time_idx=False,\n",
    "    add_target_scales=False,\n",
    "    add_encoder_length=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = training.to_dataloader(train=True, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSTM import BayesianLSTM\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "n_features = x_train.shape[1]\n",
    "\n",
    "batch_size = 128\n",
    "n_epochs = 150\n",
    "learning_rate = 0.01\n",
    "\n",
    "model = BayesianLSTM(n_features=n_features, output_length= 120, batch_size=batch_size)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "for e in range(1, n_epochs+1):\n",
    "    for X_batch, y_batch in train:\n",
    "\n",
    "        output = model(X_batch)\n",
    "        loss = criterion(output, y_batch)  \n",
    "        optimizer.zero_grad() \n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "\n",
    "    if e % 10 == 0:\n",
    "      print('epoch', e, 'loss: ', loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
